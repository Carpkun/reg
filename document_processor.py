import os
import re
from docx import Document
from typing import List, Dict, Tuple
from langchain.text_splitter import RecursiveCharacterTextSplitter

class DocumentProcessor:
    def __init__(self, chunk_size: int = 800, chunk_overlap: int = 150):
        """
        ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        
        Args:
            chunk_size: í…ìŠ¤íŠ¸ ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ì¤‘ë³µ ê¸¸ì´
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def extract_text_from_docx(self, file_path: str) -> str:
        """
        docx íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            file_path: docx íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        try:
            doc = Document(file_path)
            text_content = []
            
            # ë¬¸ì„œ ì œëª© ì¶”ê°€
            if doc.core_properties.title:
                text_content.append(f"ì œëª©: {doc.core_properties.title}")
            
            # ë¬¸ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text_content.append(paragraph.text.strip())
            
            # í‘œ ë‚´ìš© ì¶”ì¶œ
            for table in doc.tables:
                for row in table.rows:
                    row_text = []
                    for cell in row.cells:
                        if cell.text.strip():
                            row_text.append(cell.text.strip())
                    if row_text:
                        text_content.append(" | ".join(row_text))
            
            full_text = "\n".join(text_content)
            return self.clean_text(full_text)
            
        except Exception as e:
            print(f"ë¬¸ì„œ ì½ê¸° ì˜¤ë¥˜ ({file_path}): {str(e)}")
            return ""
    
    def clean_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì •ì œ
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            ì •ì œëœ í…ìŠ¤íŠ¸
        """
        # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r' +', ' ', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def process_documents(self, data_folder: str) -> List[Dict[str, str]]:
        """
        data í´ë”ì˜ ëª¨ë“  docx íŒŒì¼ ì²˜ë¦¬
        
        Args:
            data_folder: ë¬¸ì„œê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ
            
        Returns:
            ì²˜ë¦¬ëœ ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        documents = []
        
        print(f"ğŸ” [DEBUG] ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {data_folder}")
        print(f"ğŸ” [DEBUG] ì ˆëŒ€ ê²½ë¡œ: {os.path.abspath(data_folder)}")
        
        if not os.path.exists(data_folder):
            print(f"âŒ [ERROR] {data_folder} í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return documents
        
        try:
            all_files = os.listdir(data_folder)
            print(f"ğŸ” [DEBUG] í´ë” ë‚´ ì „ì²´ íŒŒì¼ ìˆ˜: {len(all_files)}")
            
            docx_files = [f for f in all_files if f.endswith('.docx')]
            print(f"ğŸ” [DEBUG] docx íŒŒì¼ ìˆ˜: {len(docx_files)}")
            
            if docx_files:
                print(f"ğŸ” [DEBUG] ì²« ë²ˆì§¸ docx íŒŒì¼: {docx_files[0]}")
                print(f"ğŸ” [DEBUG] ì²˜ìŒ 5ê°œ íŒŒì¼: {docx_files[:5]}")
            
        except Exception as e:
            print(f"âŒ [ERROR] í´ë” ì½ê¸° ì˜¤ë¥˜: {str(e)}")
            return documents
        
        if not docx_files:
            print(f"âš ï¸ [WARNING] {data_folder} í´ë”ì— docx íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return documents
        
        print(f"ğŸ“š [INFO] ì´ {len(docx_files)}ê°œì˜ docx íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        
        for i, filename in enumerate(docx_files):
            file_path = os.path.join(data_folder, filename)
            print(f"ğŸ“„ [INFO] ì²˜ë¦¬ ì¤‘ ({i+1}/{len(docx_files)}): {filename}")
            
            try:
                # íŒŒì¼ ì¡´ì¬ ë° ì½ê¸° ê¶Œí•œ í™•ì¸
                if not os.path.exists(file_path):
                    print(f"âŒ [ERROR] íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}")
                    continue
                
                if not os.access(file_path, os.R_OK):
                    print(f"âŒ [ERROR] íŒŒì¼ ì½ê¸° ê¶Œí•œ ì—†ìŒ: {file_path}")
                    continue
                
                # íŒŒì¼ í¬ê¸° í™•ì¸
                file_size = os.path.getsize(file_path)
                print(f"ğŸ” [DEBUG] íŒŒì¼ í¬ê¸°: {file_size} bytes")
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                content = self.extract_text_from_docx(file_path)
                
                if content:
                    print(f"âœ… [SUCCESS] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ, ê¸¸ì´: {len(content)} ë¬¸ì")
                    
                    # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                    chunks = self.text_splitter.split_text(content)
                    
                    for j, chunk in enumerate(chunks):
                        documents.append({
                            'content': chunk,
                            'metadata': {
                                'source': filename,
                                'chunk_id': j,
                                'file_path': file_path
                            }
                        })
                    
                    print(f"âœ… [SUCCESS] {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
                else:
                    print(f"âŒ [ERROR] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {filename}")
                    
            except Exception as e:
                print(f"âŒ [ERROR] íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({filename}): {str(e)}")
                print(f"ğŸ” [DEBUG] ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                import traceback
                print(f"ğŸ” [DEBUG] ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        
        print(f"ğŸ‰ [FINAL] ì´ {len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return documents
    
    def get_document_stats(self, documents: List[Dict[str, str]]) -> Dict[str, int]:
        """
        ë¬¸ì„œ í†µê³„ ì •ë³´ ë°˜í™˜
        
        Args:
            documents: ì²˜ë¦¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        if not documents:
            return {'ì´_ë¬¸ì„œìˆ˜': 0, 'ì´_ì²­í¬ìˆ˜': 0, 'í‰ê· _ì²­í¬ê¸¸ì´': 0}
        
        total_chunks = len(documents)
        total_length = sum(len(doc['content']) for doc in documents)
        avg_chunk_length = total_length // total_chunks if total_chunks > 0 else 0
        
        # ê³ ìœ í•œ ì†ŒìŠ¤ íŒŒì¼ ê°œìˆ˜
        unique_sources = len(set(doc['metadata']['source'] for doc in documents))
        
        return {
            'ì´_ë¬¸ì„œìˆ˜': unique_sources,
            'ì´_ì²­í¬ìˆ˜': total_chunks,
            'í‰ê· _ì²­í¬ê¸¸ì´': avg_chunk_length,
            'ì´_ë¬¸ììˆ˜': total_length
        } 